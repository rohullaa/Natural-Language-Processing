{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc67b6ee",
   "metadata": {},
   "source": [
    "# IN4080 – Natural Language Processing\n",
    "\n",
    "This assignment has two parts:\n",
    "* Part A. Sequence labeling\n",
    "* Part B. Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36575c69",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "In this part we will experiment with sequence classification and tagging. We will combine some of\n",
    "the tools for tagging from NLTK with scikit-learn to build various taggers.We will start with simple\n",
    "examples from NLTK where the tagger only considers the token to be tagged—not its context—\n",
    "and work towards more advanced logistic regression taggers (also called maximum entropy taggers).\n",
    "Finally, we will compare to some tagging algorithms installed in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05c3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb2d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features\n",
    "\n",
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents, features=pos_features):\n",
    "        self.features = features\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50686ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7915\n"
     ]
    }
   ],
   "source": [
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print(round(tagger.evaluate(test_sents), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf4d8ed",
   "metadata": {},
   "source": [
    "### 1) Tag set and baseline\n",
    "\n",
    "**Part a:** Tag set and experimental set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8119a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(tagged_sents_uni):\n",
    "    size = len(tagged_sents_uni)\n",
    "    slice_ind = round(size*10/100)\n",
    "    news_test = tagged_sents_uni[:slice_ind]\n",
    "    news_dev_test = tagged_sents_uni[slice_ind:slice_ind*2]\n",
    "    news_train = tagged_sents_uni[slice_ind*2:]\n",
    "\n",
    "    return news_test, news_dev_test,news_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4639f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sents_uni = brown.tagged_sents(categories='news',tagset = 'universal')\n",
    "news_test, news_dev_test,news_train = split_data(tagged_sents_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "895849e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8689\n"
     ]
    }
   ],
   "source": [
    "tagger_a = ConsecutivePosTagger(news_train)\n",
    "print(round(tagger_a.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c5f18",
   "metadata": {},
   "source": [
    "We got higher accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44368f1",
   "metadata": {},
   "source": [
    "**Part b:** Part b. Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cbd526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fae451cd",
   "metadata": {},
   "source": [
    "### 2) scikit-learn and tuning\n",
    "\n",
    "Our goal will be to improve the tagger compared to the simple suffix-based tagger. For the further\n",
    "experiments, we move to scikit-learn which yields more options for considering various alternatives.\n",
    "We have reimplemented the ConsecutivePosTagger to use scikit-learn classifiers below. We have\n",
    "made the classifier a parameter so that it can easily be exchanged. We start with the BernoulliNBclassifier which should correspond to the way it is done in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a634d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "class ScikitConsecutivePosTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents, features=pos_features, clf = BernoulliNB()):\n",
    "        # Using pos_features as default.\n",
    "        self.features = features\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        for tagged_sent in train_sents:\n",
    "            history = []\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_features.append(featureset)\n",
    "                train_labels.append(tag)\n",
    "                history.append(tag)\n",
    "        v = DictVectorizer()\n",
    "        X_train = v.fit_transform(train_features)\n",
    "        y_train = np.array(train_labels)\n",
    "        clf.fit(X_train, y_train)\n",
    "        self.classifier = clf\n",
    "        self.dict = v\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        test_features = []\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            test_features.append(featureset)\n",
    "        X_test = self.dict.transform(test_features)\n",
    "        tags = self.classifier.predict(X_test)\n",
    "        return zip(sentence, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82070fc",
   "metadata": {},
   "source": [
    "**Part a)** Training the ScikitConsecutivePosTagger with *news_train* set and test on the *news_dev_test* set\n",
    "with the *pos_features*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0dfa563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.857\n"
     ]
    }
   ],
   "source": [
    "tagger_scikit = ScikitConsecutivePosTagger(news_train)\n",
    "print(round(tagger_scikit.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347e86a",
   "metadata": {},
   "source": [
    "We can see that, by using the same data and same features we get a bit inferior results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a677b",
   "metadata": {},
   "source": [
    "**Part b)** One explanation could be that the smoothing is too strong. *BernoulliNB()* from scikit-learn uses Laplace smoothing as default (“add-one”). The smoothing is generalized to Lidstone smoothing which is expressed by the alpha parameter to *BernoulliNB(alpha=…)*. Therefore, we will tune the alpha parameter to find the most optimal one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97929059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tunning_bernoulli(pos_features):\n",
    "    alphas = [1, 0.5, 0.1, 0.01, 0.001, 0.0001]\n",
    "    accuracies = []\n",
    "    for alpha in alphas:\n",
    "        tagger_sci = ScikitConsecutivePosTagger(news_train,features = pos_features ,clf = BernoulliNB(alpha=alpha))\n",
    "        accuracies.append(round(tagger_sci.evaluate(news_dev_test), 4))\n",
    "    \n",
    "    return alphas,accuracies\n",
    "    \n",
    "def visualize_results(alphas, accuracies):\n",
    "    acc_alphas = {'alpha':alphas,'Accuracies':accuracies}\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(acc_alphas)\n",
    "    print(df)\n",
    "\n",
    "    best_acc = max(accuracies)\n",
    "    best_ind = accuracies.index(max(accuracies))\n",
    "    best_alpha = alphas[best_ind]\n",
    "    print(\"\")\n",
    "    print(f'Best alpha: {best_alpha} - accuracy: {best_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6f3ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas,accuracies = tunning_bernoulli(pos_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "021264cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    alpha  Accuracies\n",
      "0  1.0000      0.8787\n",
      "1  0.5000      0.8859\n",
      "2  0.1000      0.8706\n",
      "3  0.0100      0.8715\n",
      "4  0.0010      0.8643\n",
      "5  0.0001      0.8616\n",
      "\n",
      "Best alpha: 0.5 - accuracy: 0.8859\n"
     ]
    }
   ],
   "source": [
    "visualize_results(alphas, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f4d10",
   "metadata": {},
   "source": [
    "We can see that we get a little bit better result with Scikits BernoulliNB with the best alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee7fbb9",
   "metadata": {},
   "source": [
    "**Part c)** To improve the results, we may change the feature selector or the machine learner. We start with\n",
    "a simple improvement of the feature selector. The NLTK selector considers the previous word, but\n",
    "not the word itself. Intuitively, the word itself should be a stronger feature. By extending the NLTK\n",
    "feature selector with a feature for the token to be tagged, we try to find the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b08aee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features_tagged(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}    \n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        \n",
    "    #same structure, but included the token to be tagged.\n",
    "    features['tagged_word'] = sentence[i]\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b09ff02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    alpha  Accuracies\n",
      "0  1.0000      0.8985\n",
      "1  0.5000      0.9263\n",
      "2  0.1000      0.9272\n",
      "3  0.0100      0.9371\n",
      "4  0.0010      0.9416\n",
      "5  0.0001      0.9380\n",
      "\n",
      "Best alpha: 0.001 - accuracy: 0.9416\n"
     ]
    }
   ],
   "source": [
    "alphas_tag,accuracies_tag = tunning_bernoulli(pos_features_tagged)\n",
    "visualize_results(alphas_tag, accuracies_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a023f82c",
   "metadata": {},
   "source": [
    "### 3) Logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b296e71d",
   "metadata": {},
   "source": [
    "**Part a)** We proceed with the best feature selector from the last exercise. We will study the effect of the\n",
    "learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93bd3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#increased the max_iter from default 100 to 500 in order to make it converge:\n",
    "logClf = LogisticRegression(max_iter = 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca46bec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic accuracy = 0.9066\n"
     ]
    }
   ],
   "source": [
    "tagger_log = ScikitConsecutivePosTagger(news_train,features = pos_features ,clf = logClf)\n",
    "acc_log = (round(tagger_log.evaluate(news_dev_test), 4))\n",
    "print(f'Logistic accuracy = {acc_log}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706f098",
   "metadata": {},
   "source": [
    "The *Logistic Regression* classifier is better than all of the *BernoulliNB* methods without the token to be tagged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ead972",
   "metadata": {},
   "source": [
    "**Part b)** Similarly to the Naive Bayes classifier, we will study the effect of smoothing. Smoothing for LogisticRegression is done by regularization. In scikit-learn, regularization is expressed by the parameter C. A smaller C means a heavier smoothing (C is the inverse of the parameter $\\alpha$ in the lectures). We will tune the C parameter in order to find the most optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4200addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tunning_logistic(pos_features):\n",
    "    C_values = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "    accuracies = []\n",
    "    for C in C_values:\n",
    "        print(f\"Running: LogisticRegression(C = {C})\")\n",
    "        logClf = LogisticRegression(C=C,max_iter = 10000) \n",
    "        tagger_log = ScikitConsecutivePosTagger(news_train,features = pos_features ,clf = logClf)\n",
    "        accuracies.append(round(tagger_log.evaluate(news_dev_test), 4))\n",
    "    \n",
    "    return C_values,accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66691bd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: LogisticRegression(C = 0.01)\n",
      "Running: LogisticRegression(C = 0.1)\n",
      "Running: LogisticRegression(C = 1.0)\n",
      "Running: LogisticRegression(C = 10.0)\n",
      "Running: LogisticRegression(C = 100.0)\n",
      "Running: LogisticRegression(C = 1000.0)\n"
     ]
    }
   ],
   "source": [
    "C_values,accuracies_log = tunning_logistic(pos_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6199346c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     alpha  Accuracies\n",
      "0     0.01      0.8589\n",
      "1     0.10      0.9066\n",
      "2     1.00      0.9066\n",
      "3    10.00      0.9084\n",
      "4   100.00      0.8895\n",
      "5  1000.00      0.8832\n",
      "\n",
      "Best alpha: 10.0 - accuracy: 0.9084\n"
     ]
    }
   ],
   "source": [
    "visualize_results(C_values, accuracies_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee534977",
   "metadata": {},
   "source": [
    "### 4) Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c7aa7",
   "metadata": {},
   "source": [
    "**Part a)** We will now stick to the LogisticRegression() with the optimal C from the last point and see\n",
    "whether we are able to improve the results further by extending the feature extractor with more\n",
    "features. First, try adding a feature for the next word in the sentence, and then train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09f69dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features_extended(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}    \n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        \n",
    "    #next word in the secquence:\n",
    "    if i == len(sentence) - 1:\n",
    "        features['next-word'] = sentence[i]\n",
    "    else:\n",
    "        features['next-word'] = sentence[i+1]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05f83a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accuracy(pos_features,news_train,news_dev_test):\n",
    "    best_ind = accuracies_log.index(max(accuracies_log))\n",
    "    optimal_C = C_values[best_ind]\n",
    "\n",
    "    clf = LogisticRegression(C=optimal_C,solver= 'liblinear')\n",
    "    tagger = ScikitConsecutivePosTagger(news_train,features = pos_features ,clf = clf)\n",
    "    acc = (round(tagger.evaluate(news_dev_test), 4))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b952738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression with optimal C: 0.9281\n"
     ]
    }
   ],
   "source": [
    "acc_opt_log = find_accuracy(pos_features_extended,news_train,news_dev_test)\n",
    "print(f'Logistic regression with optimal C: {acc_opt_log}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de592e09",
   "metadata": {},
   "source": [
    "**Part b)** We will continue to add more features to get an even better tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7031e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features_decapilized(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}    \n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        \n",
    "    #next word in the secquence:\n",
    "    if i == len(sentence) - 1:\n",
    "       features['next-word'] = sentence[i]\n",
    "    else:\n",
    "        features['next-word'] = sentence[i+1]\n",
    "    features['current-word'] = sentence[i]  \n",
    "\n",
    "    \n",
    "    punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~—'\n",
    "    \n",
    "    s = sentence[i]\n",
    "\n",
    "    if s.isupper():\n",
    "        s = s.lower()\n",
    "    elif s.isdigit():\n",
    "        features['type'] = 'digit'\n",
    "    elif s in punctuation: \n",
    "        features['type'] = 'punctuation'\n",
    "    else:\n",
    "        features['type'] = 'other'\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9a7ae08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression with optimal C: 0.9739\n"
     ]
    }
   ],
   "source": [
    "acc_extended = find_accuracy(pos_features_decapilized,news_train,news_dev_test)\n",
    "print(f'Logistic regression with optimal C: {acc_extended}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02e31ae",
   "metadata": {},
   "source": [
    "By adding the current word, we get very much more improvement.\n",
    "\n",
    "### 5) Larger corpus and evaluation\n",
    "**Part a)** We will now test our best tagger so far on the news_test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6719cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression - accuracy = 0.9777\n"
     ]
    }
   ],
   "source": [
    "acc_test_data = find_accuracy(pos_features_decapilized,news_train,news_test)\n",
    "print(f'Logistic regression - accuracy = {acc_test_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df1e00c",
   "metadata": {},
   "source": [
    "**Part b)** Now,we will use nearly the whole Brown corpus. But we will take away two categories for later evaluation: *adventure* and *hobbies*. We will also initially stay clear of *news* to be sure not to mix training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c4a64801",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = brown.categories()\n",
    "categories.remove('news')\n",
    "categories.remove('adventure')\n",
    "categories.remove('hobbies')\n",
    "tagged_sents = brown.tagged_sents(categories=categories)\n",
    "brown_data = brown.tagged_sents(categories = categories , tagset = 'unvisersal')\n",
    "\n",
    "rest_test, rest_dev_test,rest_train = split_data(brown_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7f987632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the datasets\n",
    "\n",
    "train = rest_train + news_train\n",
    "test = rest_test + news_test\n",
    "dev_test = rest_dev_test + news_dev_test\n",
    "\n",
    "## establish baseline!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d2f59",
   "metadata": {},
   "source": [
    "**Part c)** We can then build our tagger for this larger domain. By using the best setting, we will try to find the accuracy for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a92eec02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roaka001/opt/anaconda3/envs/IN4080/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "#acc_large = find_accuracy(pos_features_decapilized,train,test)\n",
    "best_ind = accuracies_log.index(max(accuracies_log))\n",
    "optimal_C = C_values[best_ind]\n",
    "\n",
    "optimal_clf = LogisticRegression(C=optimal_C,solver= 'liblinear')\n",
    "tagger_domain = ScikitConsecutivePosTagger(train,features = pos_features_decapilized ,clf = optimal_clf)\n",
    "acc_domain = (round(tagger_domain.evaluate(test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00849d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the tagger for whole domain = 0.8774\n"
     ]
    }
   ],
   "source": [
    "print(f'The accuracy for the tagger for whole domain = {acc_domain}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380e284",
   "metadata": {},
   "source": [
    "**Part d)** Now, testing the big tagger on *adventure* and *hobbies* categories of Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e2e42c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "adventures_sents = brown.tagged_sents(categories = 'adventure' , tagset = 'unvisersal')\n",
    "hobbies_sents = brown.tagged_sents(categories = 'hobbies' , tagset = 'unvisersal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6aeafdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_adventure = (round(tagger_domain.evaluate(adventures_sents), 4))\n",
    "acc_hobbies = (round(tagger_domain.evaluate(hobbies_sents), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8be25dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9919 - adventures\n",
      "Accuracy: 0.9824 - hobbies\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {acc_adventure} - adventures')\n",
    "print(f'Accuracy: {acc_hobbies} - hobbies')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4562300",
   "metadata": {},
   "source": [
    "Describe the difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9096bc6",
   "metadata": {},
   "source": [
    "### 6) Comparing to other taggers\n",
    "\n",
    "**Part a)** NLTK comes with an HMM-tagger\n",
    "which we may train and test on our own corpus. It can be trained and testet by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "924cd082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The news HMM tagger accuracy: 0.8995\n"
     ]
    }
   ],
   "source": [
    "news_hmm_tagger = nltk.HiddenMarkovModelTagger.train(news_train)\n",
    "news_hmm_acc = round(news_hmm_tagger.evaluate(news_test), 4)\n",
    "print(f\"The news HMM tagger accuracy: {news_hmm_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dcfb33",
   "metadata": {},
   "source": [
    "Training and testing on the whole data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "021f3705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The HMM tagger accuracy: 0.6738\n"
     ]
    }
   ],
   "source": [
    "big_hmm_tagger = nltk.HiddenMarkovModelTagger.train(train)\n",
    "big_hmm_acc = round(big_hmm_tagger.evaluate(test), 4)\n",
    "print(f\"The HMM tagger accuracy: {big_hmm_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7585c068",
   "metadata": {},
   "source": [
    "This method of tagging has better speed for training og evaluating, however the accuracy is not quite good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b87ff4",
   "metadata": {},
   "source": [
    "**Part b)** NLTK also comes with an averaged perceptron tagger which we may train and test. It is currently\n",
    "considered the best tagger included with NLTK. It can be trained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b201676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_per_tagger(train,test,name):\n",
    "    per_tagger = nltk.PerceptronTagger(load=False)\n",
    "    per_tagger.train(train)\n",
    "    per_acc = round(per_tagger.evaluate(test), 4)\n",
    "\n",
    "    print(f'Perceptron tagger accuracy: {per_acc} - {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1ad0747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron tagger accuracy: 0.9649 - news_data\n",
      "Perceptron tagger accuracy: 0.9647 - all_data\n"
     ]
    }
   ],
   "source": [
    "run_per_tagger(news_train,news_test,'news_data')\n",
    "run_per_tagger(news_train,news_test,'all_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d1e16",
   "metadata": {},
   "source": [
    "This is definitely the tagger in this assignment, both in terms of speed and accuracy. It got much better results for *train* data than the best tagger above, but did the computing in much less time. However, it did not as good accuracy as the best model for the *news_data*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6361c8",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb315f9",
   "metadata": {},
   "source": [
    "In this part we will use the gensim package to familiarize ourselves with word embeddings and\n",
    "**word2vec**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e31c43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import gensim.downloader as api \n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed3b7ff",
   "metadata": {},
   "source": [
    "### 1) Basics\n",
    "**a)** The amount of different words in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b40550ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the model: 3000000\n"
     ]
    }
   ],
   "source": [
    "total_words = len(wv)\n",
    "print(f'Total words in the model: {total_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "00c57969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'cameroon' does not appear in this model\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vec_cameroon = wv['cameroon']\n",
    "except KeyError:\n",
    "    print(\"The word 'cameroon' does not appear in this model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f63a4e",
   "metadata": {},
   "source": [
    "**b)** ) Implementing a function for calculating the norm (the length) of an (embedding) vector, and a function for calculating the cosine between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8f43d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def norm(vector):\n",
    "    return np.linalg.norm(vector)\n",
    "\n",
    "def similarity(vector1, vector2):\n",
    "    cosine = np.dot(vector1,vector2)/(norm(vector1) * norm(vector2))\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b92a44",
   "metadata": {},
   "source": [
    "**c)** Comparing the functions with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b6f56f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6510957\n",
      "0.6510957\n"
     ]
    }
   ],
   "source": [
    "print(wv.similarity('king','queen'))\n",
    "print(similarity(wv['king'],wv['queen']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5bc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
