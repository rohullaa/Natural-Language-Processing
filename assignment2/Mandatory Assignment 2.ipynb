{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc67b6ee",
   "metadata": {},
   "source": [
    "# IN4080 – Natural Language Processing\n",
    "\n",
    "This assignment has two parts:\n",
    "* Part A. Sequence labeling\n",
    "* Part B. Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36575c69",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "In this part we will experiment with sequence classification and tagging. We will combine some of\n",
    "the tools for tagging from NLTK with scikit-learn to build various taggers.We will start with simple\n",
    "examples from NLTK where the tagger only considers the token to be tagged—not its context—\n",
    "and work towards more advanced logistic regression taggers (also called maximum entropy taggers).\n",
    "Finally, we will compare to some tagging algorithms installed in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05c3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb2d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features\n",
    "\n",
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents, features=pos_features):\n",
    "        self.features = features\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50686ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7915\n"
     ]
    }
   ],
   "source": [
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print(round(tagger.evaluate(test_sents), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf4d8ed",
   "metadata": {},
   "source": [
    "### 1) Tag set and baseline\n",
    "\n",
    "**Part a:** Tag set and experimental set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8119a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(tagged_sents_uni):\n",
    "    size = len(tagged_sents_uni)\n",
    "    slice_ind = round(size*10/100)\n",
    "    news_test = tagged_sents_uni[:slice_ind]\n",
    "    news_dev_test = tagged_sents_uni[slice_ind:slice_ind*2]\n",
    "    news_train = tagged_sents_uni[slice_ind*2:]\n",
    "\n",
    "    return news_test, news_dev_test,news_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4639f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sents_uni = brown.tagged_sents(categories='news',tagset = 'universal')\n",
    "news_test, news_dev_test,news_train = split_data(tagged_sents_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895849e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8689\n"
     ]
    }
   ],
   "source": [
    "tagger_a = ConsecutivePosTagger(news_train)\n",
    "print(round(tagger_a.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c5f18",
   "metadata": {},
   "source": [
    "We got higher accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44368f1",
   "metadata": {},
   "source": [
    "**Part b:** One of the first things we should do in an experiment like this, is to establish a reasonable baseline.\n",
    "A reasonable baseline here is the Most Frequent Class baseline. Each word which is seen during\n",
    "training should get its most frequent tag from the training. For words not seen during training, we\n",
    "simply use the most frequent overall tag. For this task, we can use a simple UnigramTagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cbd526",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_tagger = nltk.UnigramTagger(news_train)\n",
    "round(baseline_tagger.evaluate(news_dev_test), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae451cd",
   "metadata": {},
   "source": [
    "### 2) scikit-learn and tuning\n",
    "\n",
    "Our goal will be to improve the tagger compared to the simple suffix-based tagger. For the further\n",
    "experiments, we move to scikit-learn which yields more options for considering various alternatives.\n",
    "We have reimplemented the ConsecutivePosTagger to use scikit-learn classifiers below. We have\n",
    "made the classifier a parameter so that it can easily be exchanged. We start with the BernoulliNBclassifier which should correspond to the way it is done in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a634d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "class ScikitConsecutivePosTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents, features=pos_features, clf = BernoulliNB()):\n",
    "        # Using pos_features as default.\n",
    "        self.features = features\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        for tagged_sent in train_sents:\n",
    "            history = []\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_features.append(featureset)\n",
    "                train_labels.append(tag)\n",
    "                history.append(tag)\n",
    "        v = DictVectorizer()\n",
    "        X_train = v.fit_transform(train_features)\n",
    "        y_train = np.array(train_labels)\n",
    "        clf.fit(X_train, y_train)\n",
    "        self.classifier = clf\n",
    "        self.dict = v\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        test_features = []\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            test_features.append(featureset)\n",
    "        X_test = self.dict.transform(test_features)\n",
    "        tags = self.classifier.predict(X_test)\n",
    "        return zip(sentence, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82070fc",
   "metadata": {},
   "source": [
    "**Part a)** Training the ScikitConsecutivePosTagger with *news_train* set and test on the *news_dev_test* set\n",
    "with the *pos_features*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0dfa563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.857\n"
     ]
    }
   ],
   "source": [
    "tagger_scikit = ScikitConsecutivePosTagger(news_train)\n",
    "print(round(tagger_scikit.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347e86a",
   "metadata": {},
   "source": [
    "We can see that, by using the same data and same features we get a bit inferior results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a677b",
   "metadata": {},
   "source": [
    "**Part b)** One explanation could be that the smoothing is too strong. *BernoulliNB()* from scikit-learn uses Laplace smoothing as default (“add-one”). The smoothing is generalized to Lidstone smoothing which is expressed by the alpha parameter to *BernoulliNB(alpha=…)*. Therefore, we will tune the alpha parameter to find the most optimal one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97929059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tunning_bernoulli(pos_features):\n",
    "    alphas = [1, 0.5, 0.1, 0.01, 0.001, 0.0001]\n",
    "    accuracies = []\n",
    "    for alpha in alphas:\n",
    "        tagger_sci = ScikitConsecutivePosTagger(news_train,features = pos_features ,clf = BernoulliNB(alpha=alpha))\n",
    "        accuracies.append(round(tagger_sci.evaluate(news_dev_test), 4))\n",
    "    \n",
    "    return alphas,accuracies\n",
    "    \n",
    "def visualize_results(alphas, accuracies):\n",
    "    acc_alphas = {'alpha':alphas,'Accuracies':accuracies}\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(acc_alphas)\n",
    "    print(df)\n",
    "\n",
    "    best_acc = max(accuracies)\n",
    "    best_ind = accuracies.index(max(accuracies))\n",
    "    best_alpha = alphas[best_ind]\n",
    "    print(\"\")\n",
    "    print(f'Best alpha: {best_alpha} - accuracy: {best_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6f3ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas,accuracies = tunning_bernoulli(pos_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "021264cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    alpha  Accuracies\n",
      "0  1.0000      0.8570\n",
      "1  0.5000      0.8749\n",
      "2  0.1000      0.8695\n",
      "3  0.0100      0.8683\n",
      "4  0.0010      0.8651\n",
      "5  0.0001      0.8631\n",
      "\n",
      "Best alpha: 0.5 - accuracy: 0.8749\n"
     ]
    }
   ],
   "source": [
    "visualize_results(alphas, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f4d10",
   "metadata": {},
   "source": [
    "We can see that we get a little bit better result with Scikits BernoulliNB with the best alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee7fbb9",
   "metadata": {},
   "source": [
    "**Part c)** To improve the results, we may change the feature selector or the machine learner. We start with\n",
    "a simple improvement of the feature selector. The NLTK selector considers the previous word, but\n",
    "not the word itself. Intuitively, the word itself should be a stronger feature. By extending the NLTK\n",
    "feature selector with a feature for the token to be tagged, we try to find the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b08aee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features_tagged(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}    \n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        \n",
    "    #same structure, but included the token to be tagged.\n",
    "    features['tagged_word'] = sentence[i]\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b09ff02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    alpha  Accuracies\n",
      "0  1.0000      0.8874\n",
      "1  0.5000      0.9166\n",
      "2  0.1000      0.9244\n",
      "3  0.0100      0.9303\n",
      "4  0.0010      0.9330\n",
      "5  0.0001      0.9340\n",
      "\n",
      "Best alpha: 0.0001 - accuracy: 0.934\n"
     ]
    }
   ],
   "source": [
    "alphas_tag,accuracies_tag = tunning_bernoulli(pos_features_tagged)\n",
    "visualize_results(alphas_tag, accuracies_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a023f82c",
   "metadata": {},
   "source": [
    "### 3) Logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b296e71d",
   "metadata": {},
   "source": [
    "**Part a)** We proceed with the best feature selector from the last exercise. We will study the effect of the\n",
    "learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93bd3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#increased the max_iter from default 100 to 500 in order to make it converge:\n",
    "logClf = LogisticRegression(max_iter = 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca46bec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic accuracy = 0.8996\n"
     ]
    }
   ],
   "source": [
    "tagger_log = ScikitConsecutivePosTagger(news_train,features = pos_features ,clf = logClf)\n",
    "acc_log = (round(tagger_log.evaluate(news_dev_test), 4))\n",
    "print(f'Logistic accuracy = {acc_log}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706f098",
   "metadata": {},
   "source": [
    "The *Logistic Regression* classifier is better than all of the *BernoulliNB* methods without the token to be tagged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ead972",
   "metadata": {},
   "source": [
    "**Part b)** Similarly to the Naive Bayes classifier, we will study the effect of smoothing. Smoothing for LogisticRegression is done by regularization. In scikit-learn, regularization is expressed by the parameter C. A smaller C means a heavier smoothing (C is the inverse of the parameter $\\alpha$ in the lectures). We will tune the C parameter in order to find the most optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4200addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tunning_logistic(pos_features):\n",
    "    C_values = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "    accuracies = []\n",
    "    for C in C_values:\n",
    "        print(f\"Running: LogisticRegression(C = {C})\")\n",
    "        logClf = LogisticRegression(C=C,max_iter = 10000) \n",
    "        tagger_log = ScikitConsecutivePosTagger(news_train,features = pos_features ,clf = logClf)\n",
    "        accuracies.append(round(tagger_log.evaluate(news_dev_test), 4))\n",
    "    \n",
    "    return C_values,accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66691bd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: LogisticRegression(C = 0.01)\n",
      "Running: LogisticRegression(C = 0.1)\n",
      "Running: LogisticRegression(C = 1.0)\n",
      "Running: LogisticRegression(C = 10.0)\n",
      "Running: LogisticRegression(C = 100.0)\n",
      "Running: LogisticRegression(C = 1000.0)\n"
     ]
    }
   ],
   "source": [
    "C_values,accuracies_log = tunning_logistic(pos_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6199346c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     alpha  Accuracies\n",
      "0     0.01      0.8321\n",
      "1     0.10      0.8827\n",
      "2     1.00      0.8996\n",
      "3    10.00      0.8998\n",
      "4   100.00      0.8949\n",
      "5  1000.00      0.8896\n",
      "\n",
      "Best alpha: 10.0 - accuracy: 0.8998\n"
     ]
    }
   ],
   "source": [
    "visualize_results(C_values, accuracies_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee534977",
   "metadata": {},
   "source": [
    "### 4) Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c7aa7",
   "metadata": {},
   "source": [
    "**Part a)** We will now stick to the LogisticRegression() with the optimal C from the last point and see\n",
    "whether we are able to improve the results further by extending the feature extractor with more\n",
    "features. First, try adding a feature for the next word in the sentence, and then train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09f69dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features_extended(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}    \n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        \n",
    "    #next word in the secquence:\n",
    "    if i == len(sentence) - 1:\n",
    "        features['next-word'] = sentence[i]\n",
    "    else:\n",
    "        features['next-word'] = sentence[i+1]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05f83a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accuracy(pos_features,news_train,news_dev_test):\n",
    "    best_ind = accuracies_log.index(max(accuracies_log))\n",
    "    optimal_C = C_values[best_ind]\n",
    "\n",
    "    clf = LogisticRegression(C=optimal_C,solver= 'liblinear')\n",
    "    tagger = ScikitConsecutivePosTagger(news_train,features = pos_features ,clf = clf)\n",
    "    acc = (round(tagger.evaluate(news_dev_test), 4))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b952738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression with optimal C: 0.9231\n"
     ]
    }
   ],
   "source": [
    "acc_opt_log = find_accuracy(pos_features_extended,news_train,news_dev_test)\n",
    "print(f'Logistic regression with optimal C: {acc_opt_log}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de592e09",
   "metadata": {},
   "source": [
    "**Part b)** We will continue to add more features to get an even better tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7031e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features_decapilized(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}    \n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        \n",
    "    #next word in the secquence:\n",
    "    if i == len(sentence) - 1:\n",
    "        features['next-word'] = sentence[i]\n",
    "    else:\n",
    "        features['next-word'] = sentence[i+1]\n",
    "    features['current-word'] = sentence[i]  \n",
    "\n",
    "    \n",
    "    punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~—'\n",
    "    \n",
    "    s = sentence[i]\n",
    "\n",
    "    if s.isupper():\n",
    "        s = s.lower()\n",
    "    elif s.isdigit():\n",
    "        features['type'] = 'digit'\n",
    "    elif s in punctuation: \n",
    "        features['type'] = 'punctuation'\n",
    "    else:\n",
    "        features['type'] = 'other'\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9a7ae08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression with optimal C: 0.9669\n"
     ]
    }
   ],
   "source": [
    "acc_extended = find_accuracy(pos_features_decapilized,news_train,news_dev_test)\n",
    "print(f'Logistic regression with optimal C: {acc_extended}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02e31ae",
   "metadata": {},
   "source": [
    "By adding the current word, we get very much more improvement.\n",
    "\n",
    "### 5) Larger corpus and evaluation\n",
    "**Part a)** We will now test our best tagger so far on the news_test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6719cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression - accuracy = 0.9678\n"
     ]
    }
   ],
   "source": [
    "acc_test_data = find_accuracy(pos_features_decapilized,news_train,news_test)\n",
    "print(f'Logistic regression - accuracy = {acc_test_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df1e00c",
   "metadata": {},
   "source": [
    "**Part b)** Now,we will use nearly the whole Brown corpus. But we will take away two categories for later evaluation: *adventure* and *hobbies*. We will also initially stay clear of *news* to be sure not to mix training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4a64801",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = brown.categories()\n",
    "categories.remove('news')\n",
    "categories.remove('adventure')\n",
    "categories.remove('hobbies')\n",
    "tagged_sents = brown.tagged_sents(categories=categories)\n",
    "brown_data = brown.tagged_sents(categories = categories , tagset = 'unvisersal')\n",
    "\n",
    "rest_test, rest_dev_test,rest_train = split_data(brown_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f987632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the datasets\n",
    "\n",
    "train = rest_train + news_train\n",
    "test = rest_test + news_test\n",
    "dev_test = rest_dev_test + news_dev_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0de4b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8451"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger = nltk.UnigramTagger(train)\n",
    "round(baseline_tagger.evaluate(test), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d2f59",
   "metadata": {},
   "source": [
    "**Part c)** We can then build our tagger for this larger domain. By using the best setting, we will try to find the accuracy for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a92eec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc_large = find_accuracy(pos_features_decapilized,train,test)\n",
    "best_ind = accuracies_log.index(max(accuracies_log))\n",
    "optimal_C = C_values[best_ind]\n",
    "\n",
    "optimal_clf = LogisticRegression(C=optimal_C,solver= 'liblinear',max_iter = 1000)\n",
    "tagger_domain = ScikitConsecutivePosTagger(train,features = pos_features_decapilized ,clf = optimal_clf)\n",
    "acc_domain = (round(tagger_domain.evaluate(test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00849d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the tagger for whole domain = 0.8732\n"
     ]
    }
   ],
   "source": [
    "print(f'The accuracy for the tagger for whole domain = {acc_domain}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380e284",
   "metadata": {},
   "source": [
    "**Part d)** Now, testing the big tagger on *adventure* and *hobbies* categories of Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2e42c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "adventures_sents = brown.tagged_sents(categories = 'adventure' , tagset = 'unvisersal')\n",
    "hobbies_sents = brown.tagged_sents(categories = 'hobbies' , tagset = 'unvisersal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6aeafdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_adventure = (round(tagger_domain.evaluate(adventures_sents), 4))\n",
    "acc_hobbies = (round(tagger_domain.evaluate(hobbies_sents), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8be25dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9889 - adventures\n",
      "Accuracy: 0.9776 - hobbies\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {acc_adventure} - adventures')\n",
    "print(f'Accuracy: {acc_hobbies} - hobbies')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4562300",
   "metadata": {},
   "source": [
    "We can see here that the accuracy for the *adventures* were a bit better than for the *hobbies*. One explanation for this could be that *adventures* text is written in a formel like the trainingset, while the *hobbies* contains words and phrases in subjective form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9096bc6",
   "metadata": {},
   "source": [
    "### 6) Comparing to other taggers\n",
    "\n",
    "**Part a)** NLTK comes with an HMM-tagger\n",
    "which we may train and test on our own corpus. It can be trained and testet by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "924cd082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The news HMM tagger accuracy: 0.8995\n"
     ]
    }
   ],
   "source": [
    "news_hmm_tagger = nltk.HiddenMarkovModelTagger.train(news_train)\n",
    "news_hmm_acc = round(news_hmm_tagger.evaluate(news_test), 4)\n",
    "print(f\"The news HMM tagger accuracy: {news_hmm_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dcfb33",
   "metadata": {},
   "source": [
    "Training and testing on the whole data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "021f3705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The HMM tagger accuracy: 0.6738\n"
     ]
    }
   ],
   "source": [
    "big_hmm_tagger = nltk.HiddenMarkovModelTagger.train(train)\n",
    "big_hmm_acc = round(big_hmm_tagger.evaluate(test), 4)\n",
    "print(f\"The HMM tagger accuracy: {big_hmm_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7585c068",
   "metadata": {},
   "source": [
    "This method of tagging has better speed for training og evaluating, however the accuracy is not quite good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b87ff4",
   "metadata": {},
   "source": [
    "**Part b)** NLTK also comes with an averaged perceptron tagger which we may train and test. It is currently\n",
    "considered the best tagger included with NLTK. It can be trained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b201676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_per_tagger(train,test,name):\n",
    "    per_tagger = nltk.PerceptronTagger(load=False)\n",
    "    per_tagger.train(train)\n",
    "    per_acc = round(per_tagger.evaluate(test), 4)\n",
    "\n",
    "    print(f'Perceptron tagger accuracy: {per_acc} - {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ad0747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron tagger accuracy: 0.9656 - news_data\n",
      "Perceptron tagger accuracy: 0.9658 - all_data\n"
     ]
    }
   ],
   "source": [
    "run_per_tagger(news_train,news_test,'news_data')\n",
    "run_per_tagger(news_train,news_test,'all_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d1e16",
   "metadata": {},
   "source": [
    "This is definitely the tagger in this assignment, both in terms of speed and accuracy. It got much better results for *train* data than the best tagger above, but did the computing in much less time. However, it did not as good accuracy as the best model for the *news_data*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6361c8",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb315f9",
   "metadata": {},
   "source": [
    "In this part we will use the gensim package to familiarize ourselves with word embeddings and\n",
    "**word2vec**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e31c43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import gensim.downloader as api \n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed3b7ff",
   "metadata": {},
   "source": [
    "### 1) Basics\n",
    "**a)** The amount of different words in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b40550ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the model: 3000000\n"
     ]
    }
   ],
   "source": [
    "total_words = len(wv)\n",
    "print(f'Total words in the model: {total_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00c57969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'cameroon' does not appear in this model\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vec_cameroon = wv['cameroon']\n",
    "except KeyError:\n",
    "    print(\"The word 'cameroon' does not appear in this model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f63a4e",
   "metadata": {},
   "source": [
    "**b)** ) Implementing a function for calculating the norm (the length) of an (embedding) vector, and a function for calculating the cosine between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f43d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def norm(vector):\n",
    "    return np.linalg.norm(vector)\n",
    "\n",
    "def similarity(vector1, vector2):\n",
    "    cosine = np.dot(vector1,vector2)/(norm(vector1) * norm(vector2))\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b92a44",
   "metadata": {},
   "source": [
    "**c)** Comparing the functions with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6f56f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6510957\n",
      "0.6510957\n"
     ]
    }
   ],
   "source": [
    "print(wv.similarity('king','queen'))\n",
    "print(similarity(wv['king'],wv['queen']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c857a4c",
   "metadata": {},
   "source": [
    "### 2) Built in functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ec2dcf",
   "metadata": {},
   "source": [
    "Several built-in functions let you inspect semantic properties of the embeddings. The *most_similar*\n",
    "lets you find the nearest neighbor to one or more words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a50041fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vehicle', 0.7821096181869507), ('cars', 0.7423830032348633), ('SUV', 0.7160962224006653), ('minivan', 0.6907036304473877), ('truck', 0.6735789775848389)]\n",
      "[('SUV', 0.8532191514968872), ('vehicle', 0.8175783753395081), ('pickup_truck', 0.7763689160346985), ('Jeep', 0.7567334175109863), ('Ford_Explorer', 0.7565719485282898)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar('car', topn=5))\n",
    "print(wv.most_similar(positive=['car', 'minivan'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e9c980",
   "metadata": {},
   "source": [
    "It is also the tool for testing analogies, e.g. \"Norway is to Oslo as Sweden is to …\" as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0d3a6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Stockholm', 0.7886312007904053), ('Helsinki', 0.648445725440979), ('Stockholm_Sweden', 0.6368898153305054), ('Malmö', 0.6361426711082458), ('Oslo_Norway', 0.6240758299827576)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar(positive=['Oslo', 'Sweden'],negative = ['Norway'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f41b54",
   "metadata": {},
   "source": [
    "**a)** \n",
    "Trying these analogy tests:\n",
    "\n",
    " “ king is to man as queen is to …”\n",
    " \n",
    " “ king is to queen as man is to …”\n",
    " \n",
    " “cat is to kitten as dog is to …”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5df20da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.7609436511993408), ('girl', 0.6139993667602539), ('teenage_girl', 0.6040961742401123), ('teenager', 0.5825759172439575), ('lady', 0.5752554535865784)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar(positive=['man', 'queen'],negative = ['king'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f7c4630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.7609436511993408), ('girl', 0.6139993667602539), ('teenage_girl', 0.6040961742401123), ('teenager', 0.5825759172439575), ('lady', 0.5752554535865784)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar(positive=['queen', 'man'],negative = ['king'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a42aef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('puppy', 0.7699725031852722), ('pup', 0.6861710548400879), ('pit_bull', 0.6776558756828308), ('dogs', 0.6770986318588257), ('Rottweiler', 0.6646621823310852)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar(positive=['kitten', 'dog'],negative = ['cat'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293a65c",
   "metadata": {},
   "source": [
    "**b)** To understand the method better, we can try to follow the recipe more directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3856087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = wv['king'] + wv['woman'] - wv['man'] \n",
    "strings = ['king','queen','man','women']\n",
    "\n",
    "similarity_dict = {s:similarity(a,wv[s]) for s in strings }\n",
    "similarity_wv = wv.similar_by_vector(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13595b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('king', 0.8449392318725586), ('queen', 0.7300518155097961), ('monarch', 0.645466148853302), ('princess', 0.6156251430511475), ('crown_prince', 0.5818676948547363), ('prince', 0.5777117609977722), ('kings', 0.5613664388656616), ('sultan', 0.5376776456832886), ('Queen_Consort', 0.5344247221946716), ('queens', 0.5289887189865112)]\n"
     ]
    }
   ],
   "source": [
    "print(similarity_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc289ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'king': 0.84493923, 'queen': 0.73005176, 'man': 0.121606365, 'women': 0.25710744}\n"
     ]
    }
   ],
   "source": [
    "print(similarity_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a18982",
   "metadata": {},
   "source": [
    "This illustrates how the *most_similar* works. We tried for *a* here, meaning that we wanted to know with word most near the 'queen'. By using the cosine between the vectors, we can see above that the 'women' with '0.25710' is most near."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561e419",
   "metadata": {},
   "source": [
    "**c)** **doesnt_match:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1097acb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spain\n"
     ]
    }
   ],
   "source": [
    "print(wv.doesnt_match(['Norway', 'Denmark', 'Finland','Sweden', 'Spain', 'Stockholm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c4e8dfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Somalia\n"
     ]
    }
   ],
   "source": [
    "print(wv.doesnt_match(['Oslo', 'Bergen', 'Trondheim', 'Ålesund', 'Somalia']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38eadd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping\n"
     ]
    }
   ],
   "source": [
    "print(wv.doesnt_match(['runing','swimming','sprinting', 'sleeping','bodybuilding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad7b5585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libya\n"
     ]
    }
   ],
   "source": [
    "#It can not classify the African countries correctly.\n",
    "print(wv.doesnt_match(['Kenya','Somalia','Libya','Egypt', 'Indonesia'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52c2ed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "school\n"
     ]
    }
   ],
   "source": [
    "print(wv.doesnt_match(['book','read','potato','school','coffee'])) \n",
    "#potato most likely does not match here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e37487",
   "metadata": {},
   "source": [
    "### 3) Training a toy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "96281cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "065c8b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "sentences = brown.sents()\n",
    "model = gensim.models.Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d9a26",
   "metadata": {},
   "source": [
    "The Brown corpus is relatively smaller corpus compared to Google News corpus. Brown corpus contains around 1 million words while Google News contains approximately 100 billion words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7591644c",
   "metadata": {},
   "source": [
    "**b)** Comparing Brown model to the 'word2vec-google-news-300'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7aecec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_car = wv.most_similar('car', topn=10)\n",
    "google_queen = wv.most_similar('queen', topn=10)\n",
    "\n",
    "brown_car = model.wv.most_similar('car', topn=10)\n",
    "brown_queen = model.wv.most_similar('queen', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "81fc7240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('house', 0.9468015432357788), ('hall', 0.904809296131134), ('room', 0.9046733379364014), ('corner', 0.9016127586364746), ('town', 0.9007890820503235), ('road', 0.894270122051239), ('desk', 0.8923514485359192), ('bed', 0.8916454911231995), ('jig', 0.8833274841308594), ('ball', 0.8798673748970032)]\n",
      "[('vehicle', 0.7821096181869507), ('cars', 0.7423830032348633), ('SUV', 0.7160962224006653), ('minivan', 0.6907036304473877), ('truck', 0.6735789775848389), ('Car', 0.6677608489990234), ('Ford_Focus', 0.6673202514648438), ('Honda_Civic', 0.6626849174499512), ('Jeep', 0.651133120059967), ('pickup_truck', 0.6441437602043152)]\n"
     ]
    }
   ],
   "source": [
    "print(brown_car)\n",
    "print(google_car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e79c73eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('calf', 0.9515677690505981), ('gown', 0.9503137469291687), ('Sloan', 0.9497048258781433), ('governor', 0.9474377632141113), ('gentle', 0.947225034236908), ('shock', 0.9448780417442322), ('nervous', 0.9446044564247131), ('surgeon', 0.9406843781471252), ('blonde', 0.9385437965393066), ('cigarette', 0.9380086660385132)]\n",
      "[('queens', 0.7399442791938782), ('princess', 0.7070531249046326), ('king', 0.6510956883430481), ('monarch', 0.6383602023124695), ('very_pampered_McElhatton', 0.6357026696205139), ('Queen', 0.6163408160209656), ('NYC_anglophiles_aflutter', 0.6060680150985718), ('Queen_Consort', 0.5923796892166138), ('princesses', 0.5908074975013733), ('royal', 0.5637185573577881)]\n"
     ]
    }
   ],
   "source": [
    "print(brown_queen)\n",
    "print(google_queen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a7d94",
   "metadata": {},
   "source": [
    "**c)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "425f6594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('boy', 0.8090471029281616), ('girl', 0.8031275868415833), ('woman', 0.7906173467636108), ('himself', 0.7206084132194519), ('young', 0.7140515446662903)]\n",
      "[('greeting', 0.8697917461395264), ('enrich', 0.8523256778717041), ('choose', 0.849356472492218), ('follow', 0.8472919464111328), ('begin', 0.8413951992988586)]\n",
      "[('boy', 0.8090471029281616), ('girl', 0.8031275868415833), ('woman', 0.7906173467636108), ('himself', 0.7206084132194519), ('young', 0.7140515446662903)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['man', 'queen'],negative = ['king'], topn=5))\n",
    "print(model.wv.most_similar(positive=['kitten', 'dog'],negative = ['cat'], topn=5))\n",
    "print(model.wv.most_similar(positive=['queen', 'man'],negative = ['king'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b86d8",
   "metadata": {},
   "source": [
    "### 4) Evaluation\n",
    "Gensim comes with several methods for evaluation together with standard datasets for the tests.\n",
    "Testsets can be found by the tha datapath command, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7053dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=datapath('questions-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a8267e",
   "metadata": {},
   "source": [
    "One test we may use is to see how well the model perform on the Google analogy test datset. This\n",
    "can be run by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bfa0743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation = wv.evaluate_word_analogies(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f9076216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7401448525607863"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluation[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21dcddd",
   "metadata": {},
   "source": [
    "### 5) Application\n",
    "\n",
    "We will try a simple example of applying word embeddings to an NLP task. We consider text\n",
    "classification. We will use the same movie dataset from NLTK as we used in Mandatory assignment\n",
    "1B, with the same split as we used there. Thereby, we may compare the results with the results from\n",
    "Mandatory 1. We will consider a document as a bag of words. The word order and sentence structure will be\n",
    "ignored. Each word can be represented by its embedding. But how should a document be\n",
    "represented? The easiest is to use the “semantic fingerprint”, which means representing the\n",
    "document by the average vector of its words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "870335fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "de5c1e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_movie_docs = [(movie_reviews.raw(fileid), category) for category in movie_reviews.categories() \n",
    "                  for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.seed(2920)\n",
    "random.shuffle(raw_movie_docs)\n",
    "\n",
    "##tokenizing the text and finding every words vector\n",
    "def tokenize_and_embedding(text_data):\n",
    "    data = []\n",
    "    \n",
    "    for i in range(len(text_data)):\n",
    "        embeddings = []\n",
    "        token_data = nltk.word_tokenize(text_data[i][0])\n",
    "\n",
    "        for w in token_data:\n",
    "            if w in wv:\n",
    "                embeddings.append(wv[w])\n",
    "        data.append([embeddings,text_data[i][1]])\n",
    "    return data\n",
    "\n",
    "raw_movie_docs = tokenize_and_embedding(raw_movie_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7903a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_test = raw_movie_docs[:200]\n",
    "movie_dev = raw_movie_docs[200:]\n",
    "train_data = movie_dev[:1600]\n",
    "dev_test_data = movie_dev[1600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4d30a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_target_text(text_data):\n",
    "    target = []\n",
    "    texts = []\n",
    "    for doc in text_data:\n",
    "        texts.append(doc[0])\n",
    "        target.append(doc[1])\n",
    "        \n",
    "    return target,texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0c75d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target, train_texts = split_target_text(train_data)\n",
    "dev_test_target, dev_test_texts = split_target_text(dev_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "066e698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mean(train_texts):\n",
    "    list_of_means = []\n",
    "    for i in range(len(train_texts)):\n",
    "        text = train_texts[i]\n",
    "        text = np.array(text)\n",
    "        mean_of_text = []\n",
    "        for j in range(len(text[0])):\n",
    "            mean_of_text.append(np.mean(text[:,j]))\n",
    "        list_of_means.append(mean_of_text)\n",
    "    return list_of_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ceb817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = find_mean(train_texts)\n",
    "dev_test_texts = find_mean(dev_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be612efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tunning_logistic():\n",
    "    print(\"Running logistic regression classifier: \\n\")\n",
    "    C_values = [0.01, 0.1, 1.0, 10.0, 100.0,500.0, 1000.0,2000.0]\n",
    "    for C in C_values:\n",
    "        log_reg = LogisticRegression(solver='liblinear',C=C) \n",
    "        log_reg.fit(train_texts,train_target)\n",
    "        acc = log_reg.score(dev_test_texts,dev_test_target)\n",
    "        print(f'C = {C:7.2f} - accuracy = {acc:4.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9f2d9bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running logistic regression classifier: \n",
      "\n",
      "C =    0.01 - accuracy = 0.6950\n",
      "C =    0.10 - accuracy = 0.7150\n",
      "C =    1.00 - accuracy = 0.7950\n",
      "C =   10.00 - accuracy = 0.8250\n",
      "C =  100.00 - accuracy = 0.8500\n",
      "C =  500.00 - accuracy = 0.8300\n",
      "C = 1000.00 - accuracy = 0.8250\n",
      "C = 2000.00 - accuracy = 0.8250\n"
     ]
    }
   ],
   "source": [
    "tunning_logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df07d1",
   "metadata": {},
   "source": [
    "We can see here that the best accuracy was achieved for C = 100. However, this accuracy is not as good as the accuracy found in Mandatory 1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
