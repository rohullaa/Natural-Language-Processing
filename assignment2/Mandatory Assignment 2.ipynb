{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc67b6ee",
   "metadata": {},
   "source": [
    "# IN4080 – Natural Language Processing\n",
    "\n",
    "This assignment has two parts:\n",
    "* Part A. Sequence labeling\n",
    "* Part B. Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36575c69",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "In this part we will experiment with sequence classification and tagging. We will combine some of\n",
    "the tools for tagging from NLTK with scikit-learn to build various taggers.We will start with simple\n",
    "examples from NLTK where the tagger only considers the token to be tagged—not its context—\n",
    "and work towards more advanced logistic regression taggers (also called maximum entropy taggers).\n",
    "Finally, we will compare to some tagging algorithms installed in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05c3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb2d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features\n",
    "\n",
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents, features=pos_features):\n",
    "        self.features = features\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50686ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7915\n"
     ]
    }
   ],
   "source": [
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print(round(tagger.evaluate(test_sents), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf4d8ed",
   "metadata": {},
   "source": [
    "### 1) Tag set and baseline\n",
    "\n",
    "**Part a:** Tag set and experimental set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4639f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sents_uni = brown.tagged_sents(categories='news',tagset = 'universal')\n",
    "\n",
    "slice_ind = round(size*10/100)\n",
    "news_test = tagged_sents_uni[:slice_ind]\n",
    "news_dev_test = tagged_sents_uni[slice_ind:slice_ind*2]\n",
    "news_train = tagged_sents_uni[slice_ind*2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "895849e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8751\n"
     ]
    }
   ],
   "source": [
    "tagger_a = ConsecutivePosTagger(news_train)\n",
    "print(round(tagger_a.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c5f18",
   "metadata": {},
   "source": [
    "We got higher accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44368f1",
   "metadata": {},
   "source": [
    "**Part b:** Part b. Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3f9cfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8765\n"
     ]
    }
   ],
   "source": [
    "tagger_b = ConsecutivePosTagger(news_train)\n",
    "print(round(tagger_b.evaluate(news_train), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e293d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(news_train)):\n",
    "    for word,tag in news_train[i]:\n",
    "        pass\n",
    "\n",
    "#finne frekvensen til ordene i news_train. Dersom et ord har flere frekvenser\n",
    "#bruk den frekvensen som er størst. \n",
    "#Sammenlign med news_dev_test. De ordene fra dev_test som ikke er med i train\n",
    "#gi dem frekvensen til det største frekvensen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae451cd",
   "metadata": {},
   "source": [
    "### 2) scikit-learn and tuning\n",
    "\n",
    "Our goal will be to improve the tagger compared to the simple suffix-based tagger. For the further\n",
    "experiments, we move to scikit-learn which yields more options for considering various alternatives.\n",
    "We have reimplemented the ConsecutivePosTagger to use scikit-learn classifiers below. We have\n",
    "made the classifier a parameter so that it can easily be exchanged. We start with the BernoulliNBclassifier which should correspond to the way it is done in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a634d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "class ScikitConsecutivePosTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents, features=pos_features, clf = BernoulliNB()):\n",
    "        # Using pos_features as default.\n",
    "        self.features = features\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        for tagged_sent in train_sents:\n",
    "            history = []\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_features.append(featureset)\n",
    "                train_labels.append(tag)\n",
    "                history.append(tag)\n",
    "        v = DictVectorizer()\n",
    "        X_train = v.fit_transform(train_features)\n",
    "        y_train = np.array(train_labels)\n",
    "        clf.fit(X_train, y_train)\n",
    "        self.classifier = clf\n",
    "        self.dict = v\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        test_features = []\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            test_features.append(featureset)\n",
    "        X_test = self.dict.transform(test_features)\n",
    "        tags = self.classifier.predict(X_test)\n",
    "        return zip(sentence, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82070fc",
   "metadata": {},
   "source": [
    "**Part a)** Training the ScikitConsecutivePosTagger with *news_train* set and test on the *news_dev_test* set\n",
    "with the *pos_features*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0dfa563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8787\n"
     ]
    }
   ],
   "source": [
    "tagger_scikit = ScikitConsecutivePosTagger(news_train)\n",
    "print(round(tagger_scikit.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347e86a",
   "metadata": {},
   "source": [
    "We can see that, by using the same data and same features we get a bit inferior results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a677b",
   "metadata": {},
   "source": [
    "**Part b)** One explanation could be that the smoothing is too strong. *BernoulliNB()* from scikit-learn uses Laplace smoothing as default (“add-one”). The smoothing is generalized to Lidstone smoothing which is expressed by the alpha parameter to *BernoulliNB(alpha=…)*. Therefore, we will tune the alpha parameter to find the most optimal one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97929059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_classifier(pos_features):\n",
    "    alphas = [1, 0.5, 0.1, 0.01, 0.001, 0.0001]\n",
    "    accuracies = []\n",
    "    for alpha in alphas:\n",
    "        tagger_sci = ScikitConsecutivePosTagger(news_train,features = pos_features ,clf = BernoulliNB(alpha=alpha))\n",
    "        accuracies.append(round(tagger_sci.evaluate(news_dev_test), 4))\n",
    "    \n",
    "    return alphas,accuracies\n",
    "    \n",
    "def visualize_results(alphas, accuracies):\n",
    "    acc_alphas = {'alpha':alphas,'Accuracies':accuracies}\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(acc_alphas)\n",
    "    print(df)\n",
    "\n",
    "    best_acc = max(accuracies)\n",
    "    best_ind = accuracies.index(max(accuracies))\n",
    "    best_alpha = alphas[best_ind]\n",
    "    print(\"\")\n",
    "    print(f'Best alpha: {best_alpha} - accuracy: {best_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6f3ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas,accuracies = run_classifier(pos_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "021264cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    alpha  Accuracies\n",
      "0  1.0000      0.8787\n",
      "1  0.5000      0.8859\n",
      "2  0.1000      0.8706\n",
      "3  0.0100      0.8715\n",
      "4  0.0010      0.8643\n",
      "5  0.0001      0.8616\n",
      "\n",
      "Best alpha: 0.5 - accuracy: 0.8859\n"
     ]
    }
   ],
   "source": [
    "visualize_results(alphas, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f4d10",
   "metadata": {},
   "source": [
    "We can see that we get a little bit better result with Scikits BernoulliNB with the best alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee7fbb9",
   "metadata": {},
   "source": [
    "**Part c)** To improve the results, we may change the feature selector or the machine learner. We start with\n",
    "a simple improvement of the feature selector. The NLTK selector considers the previous word, but\n",
    "not the word itself. Intuitively, the word itself should be a stronger feature. By extending the NLTK\n",
    "feature selector with a feature for the token to be tagged, we try to find the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b08aee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features_tagged(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}    \n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        \n",
    "    #same structure, but included the token to be tagged.\n",
    "    features['tagged_word'] = sentence[i]\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b09ff02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    alpha  Accuracies\n",
      "0  1.0000      0.8985\n",
      "1  0.5000      0.9263\n",
      "2  0.1000      0.9272\n",
      "3  0.0100      0.9371\n",
      "4  0.0010      0.9416\n",
      "5  0.0001      0.9380\n",
      "\n",
      "Best alpha: 0.001 - accuracy: 0.9416\n"
     ]
    }
   ],
   "source": [
    "alphas_tag,accuracies_tag = run_classifier(pos_features_tagged)\n",
    "visualize_results(alphas_tag, accuracies_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a023f82c",
   "metadata": {},
   "source": [
    "### 3) Logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b296e71d",
   "metadata": {},
   "source": [
    "**Part a)** We proceed with the best feature selector from the last exercise. We will study the effect of the\n",
    "learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93bd3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#increased the max_iter from default 100 to 500 in order to make it converge:\n",
    "logClf = LogisticRegression(max_iter = 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca46bec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic accuracy = 0.9695\n"
     ]
    }
   ],
   "source": [
    "tagger_log = ScikitConsecutivePosTagger(news_train,features = pos_features ,clf = logClf)\n",
    "acc_log = (round(tagger_log.evaluate(news_dev_test), 4))\n",
    "print(f'Logistic accuracy = {acc_log}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706f098",
   "metadata": {},
   "source": [
    "The *Logistic Regression* classifier is better than all of the *BernoulliNB* methods without the token to be tagged."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
